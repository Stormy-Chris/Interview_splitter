{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf5033e1",
   "metadata": {},
   "source": [
    "# Read in JSON file of the interview times/speech/speakers information. Parse it and split it into a text file with the times for each speaker. Then break up the video and audio into parts by speaker. Determine top 3 emotions from each split video/audio file, graph them and then save all the graphs in a sinle pdf file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0cea4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Import required libraries\n",
    "'''\n",
    "\n",
    "# for extraction from json file\n",
    "import json\n",
    "import datetime\n",
    "import time as ptime \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import everything needed to edit video clips\n",
    "# needs to be moviepy 1.0.0 - won't work with newer versions (use pip install movepy==1.0.0)\n",
    "from moviepy.editor import *\n",
    "\n",
    "# import threading to run each part of the program\n",
    "import threading\n",
    "\n",
    "# import for directory listing/searchin\n",
    "import glob, os\n",
    "\n",
    "# extraction audio libraries\n",
    "import keras\n",
    "from tensorflow.keras.models import load_model\n",
    "import librosa \n",
    "import librosa.display\n",
    "\n",
    "# import the video libraries\n",
    "\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from keras.preprocessing import image\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "\n",
    "# import the image to pdf library for final pdf output\n",
    "import img2pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7a557e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function gets the interview file name from the given json file\n",
    "'''\n",
    "def get_Interview_number(json_file):\n",
    "    # opening json file\n",
    "    f = open(json_file)\n",
    "    # construct dict from json\n",
    "    data = json.load(f)\n",
    "    interview_mp4 = data['jobName']\n",
    "    return interview_mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9cfac32",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function extracts the startime and speaker from the given json file and save in a txt file filename_times.txt. \n",
    "Also output the video that this json refers to.\n",
    "'''\n",
    "def read_output(filename):\n",
    "        \n",
    "    filename = (filename).split('.')[0]\n",
    "    #new_filename = get_Interview_number(filename)\n",
    "    #test = os.path.join(path,new_filename,new_filename+'_times.txt')\n",
    "    #print('test = ',test)\n",
    "    \n",
    "    \n",
    "    # Create an output txt file\n",
    "    \n",
    "    with open(filename+'_times.txt','w') as w:\n",
    "        with open(filename+'.json') as f:\n",
    "            data=json.loads(f.read())\n",
    "            \n",
    "            labels = data['results']['speaker_labels']['segments']\n",
    "            speaker_start_times={}\n",
    "  \n",
    "        for label in labels:\n",
    "            for item in label['items']:\n",
    "                  speaker_start_times[item['start_time']] = item['speaker_label']\n",
    "        items = data['results']['items']\n",
    "        lines = []\n",
    "        line = ''\n",
    "        time = 0\n",
    "        speaker = 'null'\n",
    "        i = 0\n",
    "      \n",
    "                # loop through all elements\n",
    "        for item in items:\n",
    "            i = i+1\n",
    "            content = item['alternatives'][0]['content']\n",
    "              # if it's starting time\n",
    "            if item.get('start_time'):\n",
    "                current_speaker = speaker_start_times[item['start_time']]\n",
    "              # in AWS output, there are types as punctuation\n",
    "            elif item['type'] == 'punctuation':\n",
    "                line = line + content\n",
    "  \n",
    "          # handle different speaker\n",
    "            if current_speaker != speaker:\n",
    "                if speaker:\n",
    "                    lines.append({'speaker':speaker, 'line':line, 'time':time})\n",
    "                line = content\n",
    "                speaker = current_speaker\n",
    "                time = item['start_time']\n",
    "            elif item['type'] != 'punctuation':\n",
    "                line = line + ' ' + content\n",
    "        lines.append({'speaker': speaker, 'line': line,'time': time})       \n",
    "         # sort the results by the time\n",
    "        sorted_lines = sorted(lines,key=lambda k: float(k['time']))\n",
    "        \n",
    "        # write into the .txt file\n",
    "        for line_data in sorted_lines:\n",
    "        \n",
    "            # rather than convert to hh:mi:ss as above does just write the number of seconds\n",
    "            line = str(line_data['time']) + ' ' + line_data.get('speaker')\n",
    "            w.write(line + '\\n')\n",
    "        \n",
    "        #print the interview the given json file relates to\n",
    "    \n",
    "    interview_mp4 = data['jobName']\n",
    "    #print('\\nThe given json file is associated with the following interview: ' + interview_mp4)\n",
    "    return interview_mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cdee19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function outputs the text as well as the times for each speaker\n",
    "'''\n",
    "def read_output_text(filename):\n",
    "    \n",
    "    #new_dir = get_Interview_number(filename)\n",
    "    #print(new_dir)\n",
    "  # example filename: audio.json\n",
    "  \n",
    "  # take the input as the filename\n",
    "  \n",
    "    filename = (filename).split('.')[0]\n",
    "\n",
    "  # Create an output txt file\n",
    "    print(filename+'.txt')\n",
    "    with open(filename+'.txt','w') as w:\n",
    "        with open(filename+'.json') as f:\n",
    "            data=json.loads(f.read())\n",
    "            labels = data['results']['speaker_labels']['segments']\n",
    "            speaker_start_times={}\n",
    "\n",
    "        for label in labels:\n",
    "            for item in label['items']:\n",
    "                speaker_start_times[item['start_time']] = item['speaker_label']\n",
    "        items = data['results']['items']\n",
    "        lines = []\n",
    "        line = ''\n",
    "        time = 0\n",
    "        speaker = 'null'\n",
    "        i = 0\n",
    "        end_time=''\n",
    "\n",
    "      # loop through all elements\n",
    "        for item in items:\n",
    "            i = i+1\n",
    "            content = item['alternatives'][0]['content']\n",
    "              # if it's starting time\n",
    "            if item.get('start_time'):\n",
    "                current_speaker = speaker_start_times[item['start_time']]\n",
    "              # in AWS output, there are types as punctuation\n",
    "            elif item['type'] == 'punctuation':\n",
    "                line = line + content\n",
    "\n",
    "      # handle different speaker\n",
    "            if current_speaker != speaker:\n",
    "                if speaker:\n",
    "                    lines.append({'speaker':speaker, 'line':line, 'time':time})\n",
    "                line = content\n",
    "                speaker = current_speaker\n",
    "                time = item['start_time']\n",
    "                end_time = item['end_time']\n",
    "            elif item['type'] != 'punctuation':\n",
    "                line = line + ' ' + content\n",
    "        lines.append({'speaker': speaker, 'line': line,'time': time})       \n",
    "              # sort the results by the time\n",
    "        sorted_lines = sorted(lines,key=lambda k: float(k['time']))\n",
    "            #max_end_time = max(end_time)  \n",
    "              # write into the .txt file\n",
    "        for line_data in sorted_lines:\n",
    "\n",
    "            line =  str(datetime.timedelta(seconds=int(round(float(line_data['time']))))) + ' ' + line_data.get('speaker') + ': ' + line_data.get('line')\n",
    "                # rather than convert to hh:mi:ss as above does just write the number of seconds\n",
    "                #line = str(line_data['time']) + ' ' + line_data.get('speaker')\n",
    "            w.write(line + '\\n')\n",
    "        #w.write(time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57d788d",
   "metadata": {},
   "source": [
    "# Now to extract information from the txt file generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f92f0756",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Given the text file decoding of the json file this function then creates a dataframe of start and end times and well as the duration\n",
    "N.B. the final end time is calculated to using the clip.duation feature of the moviepy library\n",
    "'''\n",
    "\n",
    "def create_df(filename):\n",
    "    path, file = os.path.split(filename)\n",
    "    #print('path:',path,'file:',file)\n",
    "    \n",
    "    #global df # to allow access in the audio_video_splitting part\n",
    "    \n",
    "    # read in the decoded file and assign data to 2 columns - star_time and speaker\n",
    "    df = pd.read_csv(filename,delimiter=' ', header=None, names=['start_time','speaker'])\n",
    "    \n",
    "    # replace the '_times' from the text file\n",
    "    file_name = filename.replace('_times','') \n",
    "    # need to get interview mp4 name to check length of video\n",
    "    interview_mp4 = read_output(file_name)   \n",
    "    \n",
    "    # load the video\n",
    "    clip = VideoFileClip(os.path.join(path,interview_mp4)+'.mp4')  #path+'/'+interview_mp4+'.mp4') \n",
    "    \n",
    "    # length of video \n",
    "    length_of_video = clip.duration    \n",
    "    #print('length of video is', length_of_video)\n",
    "    \n",
    "    # add an end time and duration column and set all values to 0\n",
    "    df['end_time']=0.0\n",
    "    df['duration']=0.0\n",
    "    \n",
    "    # the end time is then simply the start time of the next speaker (until the last time)\n",
    "    for i in range(len(df)-1):\n",
    "        df['end_time'][i] = df['start_time'][i+1] \n",
    "    # set the last end time equal to the duration\n",
    "    df['end_time'].iloc[[-1]] = length_of_video    \n",
    "    \n",
    "    # calculate the durations as simply the end - start times.\n",
    "    for i in range (len(df)):\n",
    "        df['duration'][i] = df['end_time'][i] - df['start_time'][i]\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc446b8",
   "metadata": {},
   "source": [
    "# Now lets try extracting audio/video from the interview video based on the times in the file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0223a438",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function is given an interview in mp4 format and then based on times given in a dataframe (df)\n",
    "for each speaker splits the file into smaller video/audio for each speaker\n",
    "\n",
    "uses moviepy 1.0.0 - this is important it won't work with newer versions of movepy\n",
    "pip install moviepy==1.0.0\n",
    "'''\n",
    "\n",
    "def audio_video_splitter(new_dir,interview_mp4, decoded_file):\n",
    "    #print(\"params passed\",new_dir,interview_mp4, decoded_file)\n",
    "    # load the video\n",
    "    clip = VideoFileClip(interview_mp4) \n",
    "    \n",
    "    # create the dataframe from the *****_times.txt file given\n",
    "    df = create_df(decoded_file)    \n",
    "    \n",
    "    # make result directory with the label of the interview file\n",
    "    #print(\"\\nResults to be stored in newly created \"+new_dir+ \" directory\")\n",
    "    if not os.path.isdir(new_dir):\n",
    "        os.mkdir(new_dir)\n",
    "    \n",
    "    path, file = os.path.split(decoded_file)\n",
    "    # make path to new video include newly made directory\n",
    "    path=os.path.join(path,new_dir)\n",
    "    \n",
    "    # length of video \n",
    "    length_of_video = clip.duration    \n",
    "    #print('length of video is', length_of_video)\n",
    "    \n",
    "    j,k,l =0,0,0 # basic counters for the filenames\n",
    "\n",
    "    for i in range (len(df)):\n",
    "    \n",
    "        speaker = df['speaker'][i] # get the speaker \n",
    "        \n",
    "        # checks the duration is longer than 5s before splitting into files (if less than 5s not worth getting emotions from)\n",
    "        if(df['duration'][i]>5):\n",
    "            # check the speakers - either speaker 0,1,2 then store the video and audio separately \n",
    "            # as spk_0_, spk_1_  or spk_2_ .mp3 for audio and mp4 for video\n",
    "            if(speaker== 'spk_0'):\n",
    "                spk_0_ = clip.subclip(df['start_time'][i], df['end_time'][i])\n",
    "                spk_0_audio = spk_0_.audio\n",
    "                # write video to file\n",
    "                spk_0_vid_filename = \"spk_0_\"+str(j)+\".mp4\"\n",
    "                spk_0_.write_videofile(os.path.join(path,spk_0_vid_filename))     #path+\"spk_0_\"+str(j)+\".mp4\")\n",
    "                # write audio to file\n",
    "                spk_0_aud_filename = \"spk_0_\"+str(j)+\".mp3\"\n",
    "                spk_0_audio.write_audiofile(os.path.join(path,spk_0_aud_filename)) #path+\"spk_0_\"+str(j)+\".mp3\")            \n",
    "                j+=1 # need to increment the counter to make correct filenames\n",
    "\n",
    "            if(speaker== 'spk_1'):\n",
    "                spk_1_ = clip.subclip(df['start_time'][i], df['end_time'][i])\n",
    "                spk_1_audio = spk_1_.audio\n",
    "                # write video to file\n",
    "                spk_1_vid_filename = \"spk_1_\"+str(k)+\".mp4\"\n",
    "                spk_1_.write_videofile(os.path.join(path,spk_1_vid_filename))\n",
    "                # write audio to file\n",
    "                spk_1_aud_filename = \"spk_1_\"+str(k)+\".mp3\"\n",
    "                spk_1_audio.write_audiofile(os.path.join(path,spk_1_aud_filename))\n",
    "                k+=1  # need to increment the to counter to make correct filenames\n",
    "\n",
    "            if(speaker== 'spk_2'):\n",
    "                spk_2_ = clip.subclip(df['start_time'][i], df['end_time'][i])\n",
    "                spk_2_audio = spk_2_.audio\n",
    "                # write video to file\n",
    "                spk_2_vid_filename = \"spk_2_\"+str(l)+\".mp4\"\n",
    "                spk_2_.write_videofile(os.path.join(path,spk_2_vid_filename))\n",
    "                # write audio to file\n",
    "                spk_2_aud_filename = \"spk_2_\"+str(l)+\".mp3\"\n",
    "                spk_2_audio.write_audiofile(os.path.join(path,spk_2_aud_filename))\n",
    "                l+=1 # need to increment the counter to make correct filenames\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4b094e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Display method for showing results - given the predictions get the top 3\n",
    "'''\n",
    "\n",
    "def get_title(predictions, categories):    \n",
    "    #remove the largest element in intial array to get second greatest emotion and also the 3rd\n",
    "    pred_2 = np.delete(predictions, predictions.argmax())\n",
    "    pred_3 = np.delete(pred_2, pred_2.argmax())\n",
    "    # get 1st emotion\n",
    "    title = f\"Top Detected emotion: {categories[predictions.argmax()]} - {predictions.max() * 100:.2f}%\"\n",
    "    # get 2nd emotion\n",
    "    title_2 = f\"Second Detected emotion: {categories[pred_2.argmax()]} - {pred_2.max()* 100:.2f}%\"\n",
    "    title_3 = f\"Third Detected emotion: {categories[pred_3.argmax()]} - {pred_3.max()* 100:.2f}%\"\n",
    "    \n",
    "    return title  + \" \\n\" + title_2  + \"\\n\" + title_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b3752a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Get the top 3 emotions and store them in a dataframe for plotting\n",
    "'''\n",
    "\n",
    "def get_Values_df(predictions, categories):    \n",
    "    #remove the largest element in intial array to get second greatest emotion and also the 3rd\n",
    "    pred_2 = np.delete(predictions, predictions.argmax())\n",
    "    pred_3 = np.delete(pred_2, pred_2.argmax())\n",
    "    \n",
    "    pred_names = [categories[predictions.argmax()],categories[pred_2.argmax()],categories[pred_3.argmax()]]\n",
    "    pred_values =  [round(predictions.max()*100,0),round(pred_2.max()*100,0),round(pred_3.max()*100,0)]\n",
    "    \n",
    "    df_values = pd.DataFrame({'Emotions':pred_names,'Percentage':pred_values})\n",
    "    return df_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceefc57b",
   "metadata": {},
   "source": [
    "# Getting emotions from audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf8b6a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Get the Mel Frequency Cepstral Coefficients from the audio and store in a matrix for comparison with the model\n",
    "'''\n",
    "\n",
    "# load models\n",
    "model_audio = load_model(\"/home/chris/reesby/python/model4.h5\")\n",
    "\n",
    "# get the Mel Frequency Cepstral Coefficients from the audio\n",
    "def get_mfccs(audio, limit):\n",
    "    y, sr = librosa.load(audio)\n",
    "    a = librosa.feature.mfcc(y, sr=sr, n_mfcc = 40)\n",
    "    #print(a.shape[1],limit)\n",
    "    if a.shape[1] > limit:\n",
    "        mfccs = a[:,:limit]\n",
    "    elif a.shape[1] < limit:\n",
    "        mfccs = np.zeros((a.shape[0], limit))\n",
    "        mfccs[:, :a.shape[1]] = a\n",
    "    return mfccs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca6bff1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Given a filename get the Mel Frequency Cepstral Coefficients from the audio in that file \n",
    "then compare with the model to get the emotion from that audio and call get_plots to plot the emotions\n",
    "'''\n",
    "\n",
    "def get_Audio_emotions (filename):\n",
    "    # get the mfccs information from the given file\n",
    "    mfccs = get_mfccs(filename, model_audio.input_shape[-2])\n",
    "    # get the mfccs data into the right shape for a prediction\n",
    "    mfccs = mfccs.T.reshape(1, *mfccs.T.shape)\n",
    "    # get the prediction from the model\n",
    "    pred = model_audio.predict(mfccs)[0]\n",
    "    emotion_labels = ['fear', 'disgust', 'neutral', 'happy', 'sad', 'surprise', 'angry']\n",
    "    #print(pred)\n",
    "    df_vals_audio = get_Values_df(pred, emotion_labels)\n",
    "    # add an Audio label to the plot\n",
    "    audio = 'Audio'\n",
    "    get_plots(audio,filename, df_vals_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e4a42f",
   "metadata": {},
   "source": [
    "# Getting emotions from Video files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f46d0061",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_video = model_from_json(open(\"/home/chris/reesby/python/fer.json\", \"r\").read())\n",
    "model_video.load_weights('/home/chris/reesby/python/fer.h5') \n",
    "face_haar_cascade = cv2.CascadeClassifier('/home/chris/reesby/python/haarcascade_frontalface_default.xml')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a60c6037",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to get single screen shots from a given video then predict the emotion displayed \n",
    "in that screenshot. Sum up all emotions for length of video and sent to get_Values_df to create a \n",
    "dataframe for printing in get_plots\n",
    "'''\n",
    "def get_Video_emotions (filename):\n",
    "    # initialise emotions\n",
    "    angry,disgust,fear,happy,sad,surprise,neutral = 0,0,0,0,0,0,0\n",
    "    counter = 0\n",
    "    \n",
    "    cap = cv2.VideoCapture(filename) \n",
    "    #Continously read the frames \n",
    "    while True:\n",
    "        #read frame by frame and get return whether there is a stream or not\n",
    "        ret, frame=cap.read()\n",
    "\n",
    "        #If no frames received, then break from the loop\n",
    "        if not ret:\n",
    "            #print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "            break\n",
    "            c\n",
    "        #Change the frame to greyscale  \n",
    "        gray_image= cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        #We pass the image, scaleFactor and min neighbour\n",
    "        faces_detected = face_haar_cascade.detectMultiScale(gray_image,1.35,5)\n",
    "\n",
    "        #Draw Rectangles around the faces detected\n",
    "        for (x,y,w,h) in faces_detected:\n",
    "            cv2.rectangle(frame,(x,y), (x+w,y+h), (255,0,0), thickness=7)\n",
    "            roi_gray=gray_image[y:y+w,x:x+h]\n",
    "            roi_gray=cv2.resize(roi_gray,(48,48))\n",
    "\n",
    "            #Processes the image and adjust it to pass it to the model\n",
    "            image_pixels = tf.keras.preprocessing.image.img_to_array(roi_gray)\n",
    "            #plt.imshow(image_pixels)\n",
    "            #plt.show()\n",
    "            image_pixels = np.expand_dims(image_pixels, axis = 0)\n",
    "            image_pixels /= 255\n",
    "\n",
    "\n",
    "            #Get the prediction of the model\n",
    "            predictions = model_video.predict(image_pixels)\n",
    "            #print(predictions)\n",
    "            max_index = np.argmax(predictions[0])\n",
    "            #print(\"\\n filename\"+filename)\n",
    "            emotion_detection = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
    "            emotion_prediction = emotion_detection[max_index]\n",
    "\n",
    "            if(emotion_prediction =='angry'):\n",
    "                angry +=1\n",
    "            elif(emotion_prediction =='disgust'):\n",
    "                disgust +=1\n",
    "            elif(emotion_prediction =='fear'):\n",
    "                fear +=1\n",
    "            elif(emotion_prediction =='happy'):\n",
    "                happy +=1\n",
    "            elif(emotion_prediction =='sad'):\n",
    "                sad +=1\n",
    "            elif(emotion_prediction =='surprise'):\n",
    "                surprise +=1\n",
    "            elif(emotion_prediction =='neutral'):\n",
    "                neutral +=1\n",
    "\n",
    "        if cv2.waitKey(10) == ord('q'):\n",
    "                break\n",
    "        counter+=1\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows\n",
    "    \n",
    "    total = angry+disgust+fear+happy+sad+surprise+neutral\n",
    "    #print(total,angry,disgust,fear,happy,sad,surprise,neutral)\n",
    "    if(total == 0):\n",
    "        total=1\n",
    "  \n",
    "    # put the predictions in a list\n",
    "    preds = [round(fear/total,1),round(disgust/total,1),round(neutral/total,1),round(happy/total,1), round(sad/total,1),round(surprise/total,1),round(angry/total,1)]\n",
    "    # convert the list to an array for the get_title function - same format for audio/video output.\n",
    "    preds_array = np.array(preds)\n",
    "    emotion_labels = ['fear', 'disgust', 'neutral', 'happy', 'sad', 'surprise', 'angry']\n",
    "    # output predictionstart_new_thread wait\n",
    "    \n",
    "    # create a dataframe of predicted emotions\n",
    "    df_vals_video = get_Values_df(preds_array, emotion_labels)\n",
    "    #add a Video title to the plots\n",
    "    video = 'Video'\n",
    "    get_plots(video,filename,df_vals_video)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1aa0e6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to plot a bar chart and pie chart next to each other, takes a data frame of the results as input. \n",
    "medium is either Video or Audio. Output is saved as a jpg file (to be combined later in a pdf file)\n",
    "'''\n",
    "def get_plots(medium,filename,df_plot):\n",
    "    \n",
    "    # set up the colours for each emotion\n",
    "    colours = {'angry':'red','disgust':'purple','fear':'pink','happy':'yellow','sad':'grey','surprise':'orange','neutral':'blue'}\n",
    "    fig,(ax1,ax2) = plt.subplots(1,2)\n",
    "    ax1.set_ylim(0,100) # set the y axis to always be from 0 to 100    \n",
    "    df_plot.plot(x='Emotions',y='Percentage',kind='bar',rot=0, ax=ax1 , legend=False, color=[colours[key] for key in df_plot['Emotions']])   \n",
    "    x_axis = ax1.axes.get_xaxis() # use this to remove x axis label - it's pretty obvious what the values are\n",
    "    x_label = x_axis.get_label()\n",
    "    x_label.set_visible(False)\n",
    "    \n",
    "    df_plot.plot(x='Emotions',y='Percentage', labels=df_plot['Emotions'],kind='pie',colors=[colours[key] for key in df_plot['Emotions']], \n",
    "                  ax=ax2,legend=False, startangle=90, autopct='%1.0f%%')\n",
    "    y_axis = ax2.axes.get_yaxis()   #\n",
    "    y_axis.set_visible(False)\n",
    "    plt.show()\n",
    "    fig.suptitle(medium +'\\n'+ filename)\n",
    "    fig.savefig(filename+\".jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41475339",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Given a path to a directory combine all the emotion .jpg plots calculated above\n",
    "'''\n",
    "\n",
    "def to_PDF(interview_id, path): \n",
    "    print(\"creating PDF started\")\n",
    "    #print(\"path from to_PDF:\",os.path.join(path,interview_id+\"_all_graphs.pdf\"))\n",
    "    \n",
    "    images = sorted(glob.glob(os.path.join(path,\"*.jpg\")),key=os.path.getmtime)\n",
    "                \n",
    "    pdf_file_towrite = os.path.join(path,interview_id+\"_all_graphs.pdf\")\n",
    "    \n",
    "    print(\"pdf_file_towrite \",pdf_file_towrite)\n",
    "    with open(pdf_file_towrite, \"wb\") as f:\n",
    "        f.write(img2pdf.convert(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce358a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Given a path to all the video/audio file that have be separated. Calculate the audio/video emotions \n",
    "displayed in the .mp3 and .mp4 files.\n",
    "'''\n",
    "def get_Emotions(path):\n",
    "    #print(\"Get emotions started\")\n",
    "    path_mp4 = os.path.join(path,\"spk*.mp4\")\n",
    "    path_mp3 = os.path.join(path,\"spk*.mp3\")\n",
    "    print(\"\\n Path to mp4:\",path_mp4,\"\\n Path to mp3:\",path_mp3)\n",
    "    # get list of video files in order\n",
    "    video_list = sorted(glob.glob(path_mp4),key=os.path.getmtime)\n",
    "    # get list of audio files in order\n",
    "    audio_list = sorted(glob.glob(path_mp3),key=os.path.getmtime)\n",
    "    #print(zip(video_list,audio_list))\n",
    "\n",
    "    # combine the video and audio file together using zip \n",
    "    for vid_file, aud_file in zip(video_list,audio_list):\n",
    "        print(\"Video Emotions\\n\"+vid_file)\n",
    "        get_Video_emotions(vid_file)\n",
    "        print(\"Audio Emotions\\n\"+aud_file)\n",
    "        get_Audio_emotions(aud_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43358721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************************************************\n",
      "This program will split up a video interview given the associated .json file\n",
      "make sure the video file is in the same directory as the json file given\n",
      "*****************************************************************************\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the full path and json filename /home/chris/reesby/python/new_interviews/GMT20210217-053751_Candidate.json\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9093f754e2c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# split the filename and oath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'path:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'\\nfile:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Main driving function. Prompts for input json file. Output is new directory with audio/video split by speaker\n",
    "and graphs for audio/video emotions and all_graphs combined in a pdf.\n",
    "'''\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    print(\"*****************************************************************************\")\n",
    "    print(\"This program will split up a video interview given the associated .json file\")\n",
    "    print(\"make sure the video file is in the same directory as the json file given\")\n",
    "    print(\"*****************************************************************************\\n\")\n",
    "    \n",
    "    inp = input(\"Enter the full path and json filename\")\n",
    "    #inp = \"/home/chris/reesby/python/new_interviews/GMT20210217-053751_Candidate.json\" #GMT20210218-025527_Candidate.json\"\n",
    "    \n",
    "    # split the filename and oath\n",
    "    path, file = os.path.split(inp)\n",
    "    print('path:',path,'\\nfile:',file)\n",
    "    \n",
    "    # get the name of the interview file from the read_output function\n",
    "    interview_mp4 = read_output(inp)  \n",
    "    print('\\nThe interview file you should be using is : ' +interview_mp4+'.mp4')\n",
    "    \n",
    "    # the new directory will be names V1, V2, V* depending on the interview number\n",
    "    new_dir = interview_mp4\n",
    "    \n",
    "    # make result directory with the label of the interview file\n",
    "    print(\"\\nResults to be stored in newly created \"+interview_mp4+ \" directory\")\n",
    "    \n",
    "    # create the new path as the joining of the given path and \n",
    "    new_path = os.path.join(path,new_dir)\n",
    "    print(\"\\nNew path    \"+ new_path)\n",
    "    \n",
    "    # create new directory with label of interview number\n",
    "    if not os.path.isdir(new_path):\n",
    "        os.mkdir(new_path)\n",
    "    \n",
    "    # filename for interview file is path and mp4 joined             \n",
    "    mp4 = os.path.join(path,new_dir)+'.mp4'  \n",
    "    print(\"\\nInterview mp4 file    \"+ mp4)\n",
    "    \n",
    "    # decoded text from json file has been stored in file _times.txt so need to swap file name around\n",
    "    decoded_text = path+'/'+file.replace('.json','_times.txt')\n",
    "    print(\"\\n decoded text   \"+ decoded_text)\n",
    "    \n",
    "    # set up the thread to output text and times\n",
    "    t0 = threading.Thread(target=read_output_text, args=(inp,))\n",
    "    t0.start()    \n",
    "    \n",
    "    # set up first thread to create the _times.txt file from the json file given             \n",
    "    t1 = threading.Thread(target=read_output, args=(inp,))\n",
    "    t1.start()\n",
    "        \n",
    "    # next thread uses the _times.txt file to split up the video/audio by time.\n",
    "    t2 = threading.Thread(target=audio_video_splitter, args=(new_dir,mp4,decoded_text))\n",
    "    t2.start()\n",
    "\n",
    "    # next thread gets the emotions from the diffefiles givenrent audio/video files\n",
    "    t3 = threading.Thread(target= get_Emotions, args=(new_path,))\n",
    "    t3.start()\n",
    "    \n",
    "    t0.join()\n",
    "    t1.join()\n",
    "    t2.join()\n",
    "    t3.join()\n",
    "        \n",
    "    # final thread joins the emotion graphs into a pdf\n",
    "    t4 = threading.Thread(target=to_PDF, args=(interview_mp4,new_path))\n",
    "    t4.start()\n",
    "    \n",
    "    \n",
    "    t4.join()\n",
    "                         \n",
    "    print(\"\\nProgram complete. Combined final output is in \"+interview_mp4+\"_all_graphs.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02de2aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54a0932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
